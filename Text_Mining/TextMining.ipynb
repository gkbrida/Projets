{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP LEARNING : TRANSFORMERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de packages\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Pour éviter les avertissements\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le texte \n",
    "\n",
    "text = \"I am Amelie Poulain.  I was born in June 1974.   I lived alone with my father when I was a child.  Now I live in Montmartre. I work in a small café whose name is Les Deux Moulins. I am single, and I used to feel very lonely. I like dipping my hand into grain sacks and throwing stones on the Saint-Martin canal. One day, I dropped a plastic perfume-stopper, which dislodged a wall tile. I discovered an old metal box of childhood memorabilia. This box was hidden by a boy who lived in my apartment decades earlier.  I decide to track down the boy and return the box to him. If you know this boy, you need to come to see me in Montmartre.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(text, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " If you don't know this boy, you need to come to see me in Montmartre. If you don't know this boy, you need to come to see me in Montmartre. If you don't know this boy, you need to come to see me in Montmartre. If you don't know this boy, you need to come to see me in Montmartre. If you don't know this boy, you need to come to see me in Montmartre. If you don't know this boy, you need to come to see me in Montmartre. If you don't know this boy, you need to come to see\n"
     ]
    }
   ],
   "source": [
    "#  Génération du texte avec le parametre beams\n",
    "\n",
    "beam_output = model.generate(\n",
    "    input_ids,  \n",
    "    max_length=300, \n",
    "    num_beams=5, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 200 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True)[637 :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " It is a beautiful place, but I don't know if I will be able to find him again.\n",
      "\n",
      "I have been living here for a long time. My father, who lives in Paris, has been here since he was 14 years old. He is the only person I know who has lived here with me for more than 20 years. When I first moved here, my parents were very poor, so I had no money to pay my rent. The only way I could afford to live here was to buy a house in the suburbs of the city. In the summer, when the weather was good, we would go to the market and buy clothes for the\n"
     ]
    }
   ],
   "source": [
    "# Arreter de repéter les phrases\n",
    "\n",
    "beam_output = model.generate(\n",
    "    input_ids, \n",
    "    max_length=300, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 200 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True)[637 :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "  I am at the great university of Montmartre.  I am here to see about the memory of my father as well as the memory of our mother.  I am not a person of the state.  I am a person of the country.  I am a person of the Peace family. I am a person of the Church.  I am a person of the Church of Lebanon.  I am a national of the government of Lebanon.  I am a national of the government of Lebanon.  I am a national of the government of the United States.  I am a national of the government of the United States.  I am a national\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "\n",
    "# utiliser la température pour diminuer la sensibilité aux candidats de faible probabilité\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=300, \n",
    "    top_k=0, \n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 200 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True)[637 :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " I am already well-off.  I enjoy eating at the inn. On the other hand, one of my son has been working in the bank since June 2, 1989.   I have a few friends.  They are very good friends. The bank owner told me that I needed money that had arrived from North America. I received it from an old friend that also worked for that organization. I looked at the remnants of that work. The note from his address read: � A few efects in myself� and the laughter of this poor Tottnest arrived from North America.  I call him R. � \"R� for Saint\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "\n",
    "# désactivation de l'échantillonnage top_k et échantillonnage uniquement à partir des 92% de mots les plus probables\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=300, \n",
    "    top_p=0.92, \n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 200 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True)[637 :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "   In order to see him, you have to make sure that he doesn't see me.\n",
      "\n",
      "I don't want to live alone in my apartment in Montmartre, my home town, but I will try to live with my family in my apartment.   The reason I want to live in this little home town, which is a suburb from Lyon, is so that I might be able to get out of the town for a few years.  I will stay in the apartment and keep things together.   I like living alone in this little apartment, where I can\n",
      "  I am living in the village of Montmartre in the northern French countryside. I have two children: a daughter, a son, a niece and a niece-in-law. My sister-in-law is a nurse.  I work in the local restaurant with the owner of Café Montmartre. In November 2015, my cousin-in-law died after going into hospital.  He had just left the hospital to meet her husband, who was in the hospital, at the time. I am now living with my grandparents in France.  I love my mother and aunt\n",
      "  I tell him that I have found my brother in the ruins of a great cathedral, and he goes in with me for some kind of pilgrimage.  I tell him that this is our monastery.  He says to me, \"Why, I am Amelie Poulain.  My father died at the age of 13.  I remember him in his bed, on the left shoulder of his arms and legs.  I can hardly see him.  I don't know how old he was when he died. He was very weak, but he was always smiling. He always\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "sample_outputs = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True, \n",
    "    max_length=285, \n",
    "    top_k=20, \n",
    "    top_p=0.95, \n",
    "    num_return_sequences=3\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 200 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True))[640 :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2 avec PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "====== INITIAL ======\n",
      "I am Amelie Poulain.  I was born in June 1974.   I lived alone with my father when I was a child.  Now I live in Montmartre. I work in a small café whose name is Les Deux Moulins. I am single, and I used to feel very lonely. I like dipping my hand into grain sacks and throwing stones on the Saint-Martin canal. One day, I dropped a plastic perfume-stopper, which dislodged a wall tile. I discovered an old metal box of childhood memorabilia. This box was hidden by a boy who lived in my apartment decades earlier.  I decide to track down the boy and return the box to him. If you know this boy, you need to come to see me in Montmartre.\n",
      "=====================\n",
      "======== TEXT =======\n",
      "  I will send you a photo of me from my old age so your curiosity\n",
      "won't go out of control.  You see, it's not only the boy's face. If you are able\n",
      "to locate him, I promise you an unforgettable experience.  The boy's name is\n",
      "Louis-Philippe de Castiglioni.  You can call me \"L.P.\", since everyone knows my\n",
      "family name. I will introduce myself as \"Poulain\" to you, as a member of my\n",
      "family, who used to live in Montmartre with my father.  I have no friends, only\n",
      "family, friends who can't come to my house anymore\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Import textwrap library to display context\n",
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=80) \n",
    "\n",
    "MODEL_NAME = 'gpt2-large'\n",
    "\n",
    "## Tokenizer\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "text = \"I am Amelie Poulain.  I was born in June 1974.   I lived alone with my father when I was a child.  Now I live in Montmartre. I work in a small café whose name is Les Deux Moulins. I am single, and I used to feel very lonely. I like dipping my hand into grain sacks and throwing stones on the Saint-Martin canal. One day, I dropped a plastic perfume-stopper, which dislodged a wall tile. I discovered an old metal box of childhood memorabilia. This box was hidden by a boy who lived in my apartment decades earlier.  I decide to track down the boy and return the box to him. If you know this boy, you need to come to see me in Montmartre.\"\n",
    "\n",
    "sample_text = text\n",
    "tokenizer.encode(sample_text, return_tensors='pt')\n",
    "\n",
    "## Model\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "## generation de text\n",
    "#random.set_seed(0)\n",
    "def generate_text(initial_text, model, tokenizer, display=False):\n",
    "    # Generate text\n",
    "    encoded_input = tokenizer.encode(initial_text, return_tensors='pt')\n",
    "    outputs = model.generate(\n",
    "        encoded_input,\n",
    "        do_sample=True,\n",
    "        max_length=300,\n",
    "        top_k=20,\n",
    "        top_p=0.95,\n",
    "        temperature=1,\n",
    "        num_return_sequences=1)\n",
    "    \n",
    "    generated_text = []\n",
    "    for i, token_id in enumerate(outputs):\n",
    "        generated_text.append(tokenizer.decode(token_id, skip_special_tokens=True))\n",
    "\n",
    "    generated_text = ''.join(generated_text)\n",
    "\n",
    "    # Afficage\n",
    "    if display:\n",
    "        print('='*21)\n",
    "        print('='*6, 'INITIAL', '='*6)\n",
    "        print(initial_text)\n",
    "\n",
    "        print('='*21)\n",
    "        print('='*8, 'TEXT', '='*7)\n",
    "        print(wrapper.fill(generated_text)[637 :])\n",
    "    else:\n",
    "        return generated_text\n",
    "\n",
    ".0\n",
    "\n",
    "generate_text(text, model, tokenizer, display=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
